import torch
import numpy as np

# –ù–∞–π–ø—Ä–æ—Å—Ç—ñ—à–∏–º —Å–ø–æ—Å–æ–±–æ–º —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–µ–Ω–∑–æ—Ä–∞ —î –≤–∏–∫–ª–∏–∫ torch.empty().

x = torch.empty(3, 4)
print(type(x))
print(x)


# –ù–∞–π—á–∞—Å—Ç—ñ—à–µ –≤–∏ –∑–∞—Ö–æ—á–µ—Ç–µ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Å–≤—ñ–π —Ç–µ–Ω–∑–æ—Ä –ø–µ–≤–Ω–∏–º–∏ –∑–Ω–∞—á–µ–Ω–Ω—è–º. –¢–∏–ø–æ–≤–∏–º–∏ –≤–∏–ø–∞–¥–∫–∞–º–∏ —î –≤—Å—ñ –Ω—É–ª—ñ, —É—Å—ñ –æ–¥–∏–Ω–∏—Ü—ñ –∞–±–æ –≤–∏–ø–∞–¥–∫–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è:


zeros = torch.zeros(2, 3)
print(zeros)

ones = torch.ones(2, 3)
print(ones)

torch.manual_seed(1729)
random = torch.rand(2, 3)
print(random)

# –î–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–µ–Ω–∑–æ—Ä—É, —è–∫–∏–π –º–∞—î —Ç–∞–∫—É –∂ —Å–∞–º—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å –≤–∏–º—ñ—Ä—ñ–≤ —ñ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–º—ñ—Ä–æ–∫, —è–∫ —ñ –≤–∂–µ —ñ—Å–Ω—É—é—é—á–∏–π —Ç–µ–Ω–∑–æ—Ä, —ñ—Å–Ω—É—é—Ç—å –º–µ—Ç–æ–¥–∏ torch.*_like():

x = torch.empty(2, 2, 3)
print(x.shape)
print(x)

empty_like_x = torch.empty_like(x)
print(empty_like_x.shape)
print(empty_like_x)

zeros_like_x = torch.zeros_like(x)
print(zeros_like_x.shape)
print(zeros_like_x)

ones_like_x = torch.ones_like(x)
print(ones_like_x.shape)
print(ones_like_x)

rand_like_x = torch.rand_like(x)
print(rand_like_x.shape)
print(rand_like_x)

# –í–ª–∞—Å—Ç–∏–≤—ñ—Å—Ç—å .shape –º—ñ—Å—Ç–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ä–æ–∑–º—ñ—Ä—ñ–≤ –∫–æ–∂–Ω–æ–≥–æ –≤–∏–º—ñ—Ä—É —Ç–µ–Ω–∑–æ—Ä–∞ ‚Äî —É –Ω–∞—à–æ–º—É –≤–∏–ø–∞–¥–∫—É x —î —Ç—Ä–∏–≤–∏–º—ñ—Ä–Ω–∏–º —Ç–µ–Ω–∑–æ—Ä–æ–º —Ñ–æ—Ä–º–∏ 2 x 2 x 3.

# –¢–∞–∫–æ–∂ —Ç–µ–Ω–∑–æ—Ä–∏ –º–æ–∂–Ω–∞ —Å—Ç–≤–æ—Ä—é–≤–∞—Ç–∏ –±–µ–∑–ø–æ—Å–µ—Ä–µ–¥–Ω—å–æ –∑ –¥–∞–Ω–∏—Ö. –¢–∏–ø –¥–∞–Ω–∏—Ö –≤–∏–∑–Ω–∞—á–∞—î—Ç—å—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ.

some_constants = torch.tensor([[3.1415926, 2.71828], [1.61803, 0.0072897]])
print(some_constants)

some_integers = torch.tensor((2, 3, 5, 7, 11, 13, 17, 19))
print(some_integers)

more_integers = torch.tensor(((2, 4, 6), [3, 6, 9]))
print(more_integers)

# torch.tensor() —Å—Ç–≤–æ—Ä—é—î –∫–æ–ø—ñ—é –¥–∞–Ω–∏—Ö.

# –í—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ —Ç–∏–ø –¥–∞–Ω–∏—Ö —Ç–µ–Ω–∑–æ—Ä–∞ –º–æ–∂–Ω–∞ –∫—ñ–ª—å–∫–æ–º–∞ —Å–ø–æ—Å–æ–±–∞–º–∏:

a = torch.ones((2, 3), dtype=torch.int16)
print(a)

b = torch.rand((2, 3), dtype=torch.float64) * 20.
print(b)

c = b.to(torch.int32)
print(c)

# –ù–∞–π–ø—Ä–æ—Å—Ç—ñ—à–∏–π —Å–ø–æ—Å—ñ–± –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ —Ç–∏–ø –¥–∞–Ω–∏—Ö —Ç–µ–Ω–∑–æ—Ä–∞ ‚Äî –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –Ω–µ–æ–±–æ–≤‚Äô—è–∑–∫–æ–≤–æ–≥–æ –∞—Ä–≥—É–º–µ–Ω—Ç—É dtype –ø—ñ–¥ —á–∞—Å —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è.

# –Ü–Ω—à–∏–º —Å–ø–æ—Å–æ–±–æ–º –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è —Ç–∏–ø—É –¥–∞–Ω–∏—Ö —î –º–µ—Ç–æ–¥ .to(). –£ –∫–æ–º—ñ—Ä—Ü—ñ –≤–∏—â–µ –º–∏ —Å—Ç–≤–æ—Ä—é—î–º–æ –≤–∏–ø–∞–¥–∫–æ–≤–∏–π —Ç–µ–Ω–∑–æ—Ä b –∑ –ø–ª–∞–≤–∞—é—á–æ—é –∫–æ–º–æ—é –∑–≤–∏—á–∞–π–Ω–∏–º —Å–ø–æ—Å–æ–±–æ–º. –ü—ñ—Å–ª—è —Ü—å–æ–≥–æ –º–∏ —Å—Ç–≤–æ—Ä—é—î–º–æ c, –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—é—á–∏ b –Ω–∞ 32-—Ä–æ–∑—Ä—è–¥–Ω–µ —Ü—ñ–ª–µ —á–∏—Å–ª–æ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –º–µ—Ç–æ–¥—É .to().

# –î–æ—Å—Ç—É–ø–Ω—ñ —Ç–∏–ø–∏ –¥–∞–Ω–∏—Ö PyTorch –≤–∫–ª—é—á–∞—é—Ç—å: torch.bool, torch.int8, torch.uint8, torch.int16, torch.int32, torch.int64, torch.half, torch.float, torch.double, torch.bfloat.

tensor = torch.rand(3,4)

print(f"Shape of tensor: {tensor.shape}")
print(f"Datatype of tensor: {tensor.dtype}")
print(f"Device tensor is stored on: {tensor.device}")

# –Ü–Ω–¥–µ–∫—Å—É–≤–∞–Ω–Ω—è, –Ω–∞—Ä—ñ–∑–∫–∞, –æ–±‚Äô—î–¥–Ω–∞–Ω–Ω—è —Ç–∞ —Ç—Ä–∞–Ω—Å–ª—è—Ü—ñ—è —Ç–µ–Ω–∑–æ—Ä—ñ–≤
tensor = torch.ones(4, 4)
print(f"First row: {tensor[0]}")
print(f"First column: {tensor[:, 0]}")
print(f"Last column: {tensor[..., -1]}")
tensor[:,1] = 0
print(tensor)

# –û–±‚Äô—î–¥–Ω–∞–Ω–Ω—è —Ç–µ–Ω–∑–æ—Ä—ñ–≤

# - torch.cat –æ–±‚Äô—î–¥–Ω—É—î –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å —Ç–µ–Ω–∑–æ—Ä—ñ–≤ —É–∑–¥–æ–≤–∂ –≤–∏–º—ñ—Ä—É, —â–æ –≤–∂–µ —ñ—Å–Ω—É—î:
x = torch.tensor([[1, 2], [3, 4]])
y = torch.tensor([[5, 6]])
# –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü—ñ—è –≤–∑–¥–æ–≤–∂ 0-–≥–æ –≤–∏–º—ñ—Ä—É
result = torch.cat((x, y), dim=0)
print(result)

# - torch.stack –æ–±‚Äô—î–¥–Ω—É—î –ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å —Ç–µ–Ω–∑–æ—Ä—ñ–≤ —É –Ω–æ–≤–æ–º—É –≤–∏–º—ñ—Ä—ñ, —Å—Ç–≤–æ—Ä—é—é—á–∏ –Ω–æ–≤–∏–π –≤–∏–º—ñ—Ä –≤ –æ—Ç—Ä–∏–º–∞–Ω–æ–º—É —Ç–µ–Ω–∑–æ—Ä—ñ.

x = torch.tensor([[1, 2], [3, 4]])
y = torch.tensor([[5, 6], [3, 4]])
# –°—Ç–µ–∫ –≤–∑–¥–æ–≤–∂ –Ω–æ–≤–æ–≥–æ –≤–∏–º—ñ—Ä—É (–≤–∏–º—ñ—Ä 0)
result = torch.stack((x, y), dim=0)
print(result)

# –¢—Ä–∞–Ω—Å–ª—è—Ü—ñ—è —Ç–µ–Ω–∑–æ—Ä—ñ–≤ (tensor broadcasting)
rand = torch.rand(2, 4)
doubled = rand * (torch.ones(1, 4) * 2)

print(rand)
print(doubled)

'''
–¢—Ä–∞–Ω—Å–ª—è—Ü—ñ—è ‚Äî —Ü–µ —Å–ø–æ—Å—ñ–± –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –æ–ø–µ—Ä–∞—Ü—ñ—ó –º—ñ–∂ —Ç–µ–Ω–∑–æ—Ä–∞–º–∏, —è–∫—ñ –º–∞—é—Ç—å –ø–æ–¥—ñ–±–Ω—ñ —Ñ–æ—Ä–º–∏. –£ –Ω–∞–≤–µ–¥–µ–Ω–æ–º—É –≤–∏—â–µ –ø—Ä–∏–∫–ª–∞–¥—ñ —Ç–µ–Ω–∑–æ—Ä –∑ –æ–¥–Ω–∏–º —Ä—è–¥–∫–æ–º —ñ —á–æ—Ç–∏—Ä–º–∞ —Å—Ç–æ–≤–ø—Ü—è–º–∏ –º–Ω–æ–∂–∏—Ç—å—Å—è –Ω–∞ –æ–±–∏–¥–≤–∞ —Ä—è–¥–∫–∏ —Ç–µ–Ω–∑–æ—Ä–∞ –∑ –¥–≤–æ–º–∞ —Ä—è–¥–∫–∞–º–∏ —ñ —á–æ—Ç–∏—Ä–º–∞ —Å—Ç–æ–≤–ø—Ü—è–º–∏.

–¶–µ –≤–∞–∂–ª–∏–≤–∞ –æ–ø–µ—Ä–∞—Ü—ñ—è –≤ Deep Learning. –ó–≤–∏—á–∞–π–Ω–∏–º –ø—Ä–∏–∫–ª–∞–¥–æ–º —î –º–Ω–æ–∂–µ–Ω–Ω—è —Ç–µ–Ω–∑–æ—Ä–∞ –≤–∞–≥–æ–≤–∏—Ö –∫–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç—ñ–≤ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –≥—Ä—É–ø—É –≤—Ö—ñ–¥–Ω–∏—Ö —Ç–µ–Ω–∑–æ—Ä—ñ–≤, –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –æ–ø–µ—Ä–∞—Ü—ñ—ó –¥–æ –∫–æ–∂–Ω–æ–≥–æ –µ–∫–∑–µ–º–ø–ª—è—Ä–∞ –≤ –≥—Ä—É–ø—ñ –æ–∫—Ä–µ–º–æ —Ç–∞ –ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è —Ç–µ–Ω–∑–æ—Ä–∞ —ñ–¥–µ–Ω—Ç–∏—á–Ω–æ—ó —Ñ–æ—Ä–º–∏. –¢–∞–∫, —É –Ω–∞–≤–µ–¥–µ–Ω–æ–º—É –≤–∏—â–µ –ø—Ä–∏–∫–ª–∞–¥—ñ –º–Ω–æ–∂–µ–Ω–Ω—è —Ç–µ–Ω–∑–æ—Ä—ñ–≤ —Ä–æ–∑–º—ñ—Ä–∞–º–∏ (2, 4) —Ç–∞ (1, 4) –±—É–ª–æ –ø–æ–≤–µ—Ä–Ω—É—Ç–æ —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º–∏ (2, 4).

–ü—Ä–∞–≤–∏–ª–∞ —Ç—Ä–∞–Ω—Å–ª—è—Ü—ñ—ó:
- –ö–æ–∂–µ–Ω —Ç–µ–Ω–∑–æ—Ä –ø–æ–≤–∏–Ω–µ–Ω –º–∞—Ç–∏ –ø—Ä–∏–Ω–∞–π–º–Ω—ñ –æ–¥–∏–Ω –≤–∏–º—ñ—Ä ‚Äî –Ω—ñ—è–∫–∏—Ö –ø–æ—Ä–æ–∂–Ω—ñ—Ö —Ç–µ–Ω–∑–æ—Ä—ñ–≤.
- –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—ñ–≤ –¥–≤–æ—Ö —Ç–µ–Ω–∑–æ—Ä—ñ–≤ –≤—ñ–¥ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ –¥–æ –ø–µ—Ä—à–æ–≥–æ:
—Ä–æ–∑–º—ñ—Ä–∏ –≤–∏–º—ñ—Ä—ñ–≤ –º–∞—é—Ç—å –±—É—Ç–∏ —Ä—ñ–≤–Ω–∏–º–∏
–∞–±–æ
–æ–¥–∏–Ω —ñ–∑ –≤–∏–º—ñ—Ä—ñ–≤ –º–∞—î —Ä–æ–∑–º—ñ—Ä 1
–∞–±–æ
—Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –Ω–µ —ñ—Å–Ω—É—î –≤ –æ–¥–Ω–æ–º—É –∑ —Ç–µ–Ω–∑–æ—Ä—ñ–≤.
'''

a = torch.ones(4, 3, 2)

b = a * torch.rand(   3, 2) # 3rd & 2nd dims identical to a, dim 1 absent
print(b)

c = a * torch.rand(   3, 1) # 3rd dim = 1, 2nd dim identical to a
print(c)

d = a * torch.rand(   1, 2) # 3rd dim identical to a, 2nd dim = 1
print(d)

# –û–¥–Ω–æ–µ–ª–µ–º–µ–Ω—Ç–Ω—ñ —Ç–µ–Ω–∑–æ—Ä–∏
# –Ø–∫—â–æ —É –≤–∞—Å —î –æ–¥–Ω–æ–µ–ª–µ–º–µ–Ω—Ç–Ω–∏–π —Ç–µ–Ω–∑–æ—Ä, –æ—Ç—Ä–∏–º–∞–Ω–∏–π, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, —à–ª—è—Ö–æ–º –æ–±‚Äô—î–¥–Ω–∞–Ω–Ω—è –≤—Å—ñ—Ö –∑–Ω–∞—á–µ–Ω—å —Ç–µ–Ω–∑–æ—Ä–∞ –≤ –æ–¥–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è, –≤–∏ –º–æ–∂–µ—Ç–µ –ø–µ—Ä–µ—Ç–≤–æ—Ä–∏—Ç–∏ –π–æ–≥–æ –Ω–∞ —á–∏—Å–ª–æ–≤–µ –∑–Ω–∞—á–µ–Ω–Ω—è Python –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –º–µ—Ç–æ–¥—É item().

agg = tensor.sum()
agg_item = agg.item()
print(agg_item, type(agg_item))

# –õ–æ–∫–∞–ª—å–Ω—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó (In-place operations)
# –û–ø–µ—Ä–∞—Ü—ñ—ó, —è–∫—ñ –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –æ–ø–µ—Ä–∞–Ω–¥—ñ, —î –ª–æ–∫–∞–ª—å–Ω–∏–º–∏. –í–æ–Ω–∏ –ø–æ–∑–Ω–∞—á–∞—é—Ç—å—Å—è —Å—É—Ñ—ñ–∫—Å–æ–º _. –ù–∞–ø—Ä–∏–∫–ª–∞–¥: *x.copy*(y), x.t_() –∑–º—ñ–Ω–∏—Ç—å x.
print(f"{tensor} \\n")
tensor.add_(5)
print(tensor)

# –¢–µ–Ω–∑–æ—Ä–∏ –Ω–∞ CPU —ñ –º–∞—Å–∏–≤–∏ NumPy –º–æ–∂—É—Ç—å —Å–ø—ñ–ª—å–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –ø–∞–º‚Äô—è—Ç—å, —ñ –∑–º—ñ–Ω–∞ –æ–¥–Ω–æ–≥–æ –∑ –º–∞—Å–∏–≤—ñ–≤ –∑–º—ñ–Ω–∏—Ç—å —ñ–Ω—à–∏–π.
t = torch.ones(5)
print(f"t: {t}")
n = t.numpy()
print(f"n: {n}")

t.add_(1)
print(f"t: {t}")
print(f"n: {n}")

# –ü—Ä–∏–≤–µ–¥–µ–Ω–Ω—è –º–∞—Å–∏–≤—É NumPy –¥–æ tensor
n = np.ones(5)
t = torch.from_numpy(n)

# –ó–º—ñ–Ω–∏ –≤ –º–∞—Å–∏–≤—ñ NumPy –≤—ñ–¥–æ–±—Ä–∞–∂–∞—é—Ç—å—Å—è –≤ —Ç–µ–Ω–∑–æ—Ä—ñ.

np.add(n, 1, out=n)
print(f"t: {t}")
print(f"n: {n}")

# –ü–µ—Ä–µ–Ω–æ—Å–∏–º–æ –Ω–∞—à —Ç–µ–Ω–∑–æ—Ä –Ω–∞ –≥—Ä–∞—Ñ—ñ—á–Ω–∏–π –ø—Ä–æ—Ü–µ—Å–æ—Ä (GPU), —è–∫—â–æ –≤—ñ–Ω –¥–æ—Å—Ç—É–ø–Ω–∏–π
if torch.cuda.is_available():
    tensor = tensor.to("cuda")

    # –†–æ–±–æ—Ç–∞ –∑ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—é —Ç–µ–Ω–∑–æ—Ä—ñ–≤
# –û—Ç–∂–µ, —è–∫ –∑—Ä–æ–±–∏—Ç–∏ –±–∞—Ç—á –∑ –æ–¥–Ω–æ–≥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è?

a = torch.rand(3, 226, 226)
b = a.unsqueeze(0)

print(a.shape)
print(b.shape)

# –í–∏–≤–µ–¥–µ–Ω–Ω—è:

# torch.Size([3, 226, 226])
# torch.Size([1, 3, 226, 226])

# –ú–µ—Ç–æ–¥ unsqueeze() –¥–æ–¥–∞—î –≤–∏–º—ñ—Ä —Ä–æ–∑–º—ñ—Ä—É 1. unsqueeze(0) –¥–æ–¥–∞—î –π–æ–≥–æ —è–∫ –Ω–æ–≤–∏–π –Ω—É–ª—å–æ–≤–∏–π –≤–∏–º—ñ—Ä ‚Äî —Ç–µ–ø–µ—Ä –º–∏ –º–∞—î–º–æ –±–∞—Ç—á —Ä–æ–∑–º—ñ—Ä—É 1.

# –©–æ —Ä–æ–±–∏—Ç–∏ —É –≤–∏–ø–∞–¥–∫—É, –∫–æ–ª–∏ –≤–∞–º –∑–Ω–∞–¥–æ–±–∏—Ç—å—Å—è –∑—Ä–æ–±–∏—Ç–∏ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –∑ —Å–∞–º–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º ‚Äî 20-–µ–ª–µ–º–µ–Ω—Ç–Ω–∏–º –≤–µ–∫—Ç–æ—Ä–æ–º?

a = torch.rand(1, 20)
print(a.shape)
print(a)

b = a.squeeze(0)
print(b.shape)
print(b)

c = torch.rand(2, 2)
print(c.shape)

d = c.squeeze(0)
print(d.shape)

# üí° –í–∏–∫–ª–∏–∫–∏ squeeze() —ñ unsqueeze() –º–æ–∂—É—Ç—å –¥—ñ—è—Ç–∏ –ª–∏—à–µ –Ω–∞ –≤–∏–º—ñ—Ä–∞—Ö —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ 1, –æ—Å–∫—ñ–ª—å–∫–∏, —ñ–Ω–∞–∫—à–µ, –∑–º—ñ–Ω–∏—Ç—å—Å—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ —É —Ç–µ–Ω–∑–æ—Ä—ñ.


# –û–¥–Ω–æ–≤–∏–º—ñ—Ä–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –º–æ–∂–Ω–∞ –æ—Ç—Ä–∏–º–∞—Ç–∏ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –æ–ø–µ—Ä–∞—Ü—ñ—ó reshape() –∑–∞ —É–º–æ–≤–∏, —â–æ –≤–∏–º—ñ—Ä–∏, —è–∫—ñ –≤–∏ —Ö–æ—á–µ—Ç–µ –æ—Ç—Ä–∏–º–∞—Ç–∏, –¥–∞—é—Ç—å —Ç–∞–∫—É —Å–∞–º—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ª–µ–º–µ–Ω—Ç—ñ–≤, —è–∫—É –º–∞—î –≤—Ö—ñ–¥–Ω–∏–π —Ç–µ–Ω–∑–æ—Ä:

# üí° –ê—Ä–≥—É–º–µ–Ω—Ç (6 * 20 * 20,) —É –æ—Å—Ç–∞–Ω–Ω—å–æ–º—É —Ä—è–¥–∫—É –∫–æ–º—ñ—Ä–∫–∏ –≤–∏—â–µ –ø–æ—è—Å–Ω—é—î—Ç—å—Å—è —Ç–∏–º, —â–æ PyTorch –æ—á—ñ–∫—É—î –∫–æ—Ä—Ç–µ–∂, –∫–æ–ª–∏ –≤–∫–∞–∑—É—î —Ñ–æ—Ä–º—É —Ç–µ–Ω–∑–æ—Ä–∞, –∞–ª–µ –∫–æ–ª–∏ —Ñ–æ—Ä–º–∞ —î –ø–µ—Ä—à–∏–º –∞—Ä–≥—É–º–µ–Ω—Ç–æ–º –º–µ—Ç–æ–¥—É. –¶–µ –¥–æ–∑–≤–æ–ª—è—î –Ω–∞–º ¬´—à–∞—Ö—Ä–∞—é–≤–∞—Ç–∏¬ª —Ç–∞ –∑ –ª–µ–≥–∫—ñ—Å—Ç—é –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ —Ä—è–¥ —Ü—ñ–ª–∏—Ö —á–∏—Å–µ–ª. 
# –¢—É—Ç –Ω–∞–º –¥–æ–≤–µ–ª–æ—Å—è –¥–æ–¥–∞—Ç–∏ –¥—É–∂–∫–∏ —Ç–∞ –∫–æ–º—É, —â–æ–± –ø–µ—Ä–µ–∫–æ–Ω–∞—Ç–∏ –º–µ—Ç–æ–¥, —â–æ —Ü–µ –¥—ñ–π—Å–Ω–æ –æ–¥–Ω–æ–µ–ª–µ–º–µ–Ω—Ç–Ω–∏–π –∫–æ—Ä—Ç–µ–∂.

output3d = torch.rand(6, 20, 20)
print(output3d.shape)

input1d = output3d.reshape(6 * 20 * 20)
print(input1d.shape)

# —Ç–∞–∫–æ–∂ –º–æ–∂–µ –≤–∏–∫–ª–∏–∫–∞—Ç–∏ –π–æ–≥–æ —è–∫ –º–µ—Ç–æ–¥ —É –º–æ–¥—É–ª—ñ torch:
print(torch.reshape(output3d, (6 * 20 * 20,)).shape)

# –ó–ê–î–ê–ß–ê

from torch import nn
import torch.optim as optim
from sklearn.metrics import accuracy_score
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split

m = nn.Linear(5, 3)
input = torch.randn(4, 5)
output = m(input)

print('Input:', input, f'shape {input.shape}', sep='\\n')
print('\\nOutput:', output, f'shape {output.shape}', sep='\\n')

'''
–°–∏–≥–º–æ—ñ–¥–∞ ‚Äî —Ü–µ –Ω–∞—à–∞ —Ñ—É–Ω–∫—Ü—ñ—è –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó, —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞ –≤ PyTorch —è–∫ torch.sigmoid.

torch.sigmoid(*input*, *, *out=None)

–ü–∞—Ä–∞–º–µ—Ç—Ä–∏

input (Tensor) ‚Äì –≤—Ö—ñ–¥–Ω–∏–π —Ç–µ–Ω–∑–æ—Ä;
out (Tensor, –Ω–µ–æ–±–æ–≤‚Äô—è–∑–∫–æ–≤–æ) ‚Äì –≤–∏—Ö—ñ–¥–Ω–∏–π —Ç–µ–Ω–∑–æ—Ä. –í–∏—Ö—ñ–¥–Ω–∏–º —Ç–µ–Ω–∑–æ—Ä–æ–º –º–æ–∂–µ –±—É—Ç–∏ –∑–∞–∑–¥–∞–ª–µ–≥—ñ–¥—å —Å—Ç–≤–æ—Ä–µ–Ω–∏–π –æ–±‚Äô—î–∫—Ç torch.tensor, –≤ —è–∫–∏–π –±—É–¥–µ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É.
'''

# –í–∏–∑–Ω–∞—á–∏–º–æ –∫–ª–∞—Å –ª–æ–≥—ñ—Å—Ç–∏—á–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó.

class LogisticRegression(nn.Module):
    def __init__(self, input_dim):
        super(LogisticRegression, self).__init__()
        self.linear = nn.Linear(input_dim, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        out = self.linear(x)
        out = self.sigmoid(out)
        return out

df = pd.read_csv('data/train.csv')
df = df.set_index('PassengerId')

TARGET = 'Transported'
FEATURES = [col for col in df.columns if col != TARGET]

text_features = ["Cabin", "Name"]
cat_features = [col for col in FEATURES if df[col].nunique() < 25 and col not in text_features ]
cont_features = [col for col in FEATURES if df[col].nunique() >= 25 and col not in text_features ]

ax = df[TARGET].value_counts().plot(kind='bar', figsize=(8, 5))
for i in ax.containers:
  ax.bar_label(i)
  ax.set_xlabel("value")
  ax.set_ylabel("count")
       
plt.suptitle("Target feature distribution")

plt.tight_layout()
plt.show()


ax = df.loc[:, cont_features].hist(figsize=(10, 12), grid=False, edgecolor='black', linewidth=.4)
for row in ax:
  for col in row:
    for i in col.containers:
      col.bar_label(i)
      col.set_xlabel("value")
      col.set_ylabel("count")
     
services_features = cont_features[1:]

for feature in services_features:
    df[f'used_{feature}'] = df.loc[:, feature].apply(lambda x: 1 if x > 0 else 0)

# Correlation matrix for selected features
corr_matrix = df.loc[:, cont_features + ['CryoSleep', 'VIP', TARGET]].corr()

# Display a styled correlation matrix (optional, requires jinja2)
try:
    print(corr_matrix)
except:
    print("Install 'jinja2' to use .style on DataFrames")

imputer_cols = ["Age", "FoodCourt", "ShoppingMall", "Spa", "VRDeck" ,"RoomService"]
imputer = SimpleImputer(strategy='median')
imputer.fit(df[imputer_cols])
df[imputer_cols] = imputer.transform(df[imputer_cols])
df["HomePlanet"].fillna('Gallifrey', inplace=True)
df["Destination"].fillna('Skaro', inplace=True)

df['CryoSleep_is_missing'] = df['CryoSleep'].isna().astype(int)
df['VIP_is_missing'] = df['VIP'].isna().astype(int)

df["CryoSleep"].fillna(False, inplace=True)
df["VIP"].fillna(False, inplace=True)

df["CryoSleep"] = df["CryoSleep"].astype(int)
df["VIP"] = df["VIP"].astype(int)

dummies = pd.get_dummies(df.loc[:, ['HomePlanet', 'Destination']], dtype=int)
dummies

df = pd.concat([df, dummies], axis=1)
df.drop(columns=['HomePlanet', 'Destination'], inplace=True)

# –û—Å–∫—ñ–ª—å–∫–∏ –º–æ–¥–µ–ª—å, —è–∫—É –º–∏ –±—É–¥–µ–º–æ —Å—Ç–≤–æ—Ä—é–≤–∞—Ç–∏, –æ—á—ñ–∫—É—î –Ω–∞ —á–∏—Å–ª–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è, –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ —Ü—ñ–ª—å–æ–≤—É –æ–∑–Ω–∞–∫—É –∑ –±—ñ–Ω–∞—Ä–Ω–æ—ó –Ω–∞ —Ü—ñ–ª–æ—á–∏—Å–µ–ª—å–Ω—É.

df[TARGET] = df[TARGET].astype(int)

# –û—Å–∫—ñ–ª—å–∫–∏ –Ω–∞—Ä–∞–∑—ñ –º–∏ –Ω–µ –æ–±—Ä–æ–±–ª—è—î–º–æ —Ç–µ–∫—Å—Ç–æ–≤—ñ –∑–º—ñ–Ω–Ω—ñ, –≤–∏–¥–∞–ª–∏–º–æ —ó—Ö.

df.drop(["Name" ,"Cabin"] , axis=1 ,inplace = True)

# Train/test split

X = df.drop(TARGET , axis =1 )
y = df[TARGET]

X_train , X_test , y_train , y_test = train_test_split(X, y, random_state = 42, test_size =0.33, stratify=y)

input_dim = X_train.shape[1]
model = LogisticRegression(input_dim)

# –í–∏–∑–Ω–∞—á–∞—î–º–æ —Ñ—É–Ω–∫—Ü—ñ—é –≤—Ç—Ä–∞—Ç. –ú–æ–¥—É–ª—å nn –º—ñ—Å—Ç–∏—Ç—å –≤ —Å–æ–±—ñ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç, –≤ —Ç–æ–º—É —á–∏—Å–ª—ñ binary cross-entropy.

criterion = nn.BCELoss()

optimizer = optim.SGD(model.parameters(), lr=0.01)



num_epochs = 50
for epoch in range(num_epochs):
    # –ü–µ—Ä–µ–¥–∞—á–∞ –≤–ø–µ—Ä–µ–¥
    outputs = model(X_train)
    loss = criterion(outputs.squeeze(), y_train)
    
    # –ó–≤–æ—Ä–æ—Ç–Ω–∏–π –ø—Ä–æ—Ö—ñ–¥ —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 5 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

with torch.no_grad():
    y_pred = model(X_test).squeeze().numpy().round()

accuracy_score(y_test, y_pred)